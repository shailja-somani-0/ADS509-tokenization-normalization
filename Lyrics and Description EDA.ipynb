{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "Shailja Somani\\\n",
    "ADS 509 Summer 2024\\\n",
    "May 18, 2024\n",
    "\n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn.  If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8507453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Users/shailjasomani/opt/anaconda3/lib/python3.9/site-packages (2.11.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shailjasomani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the emoji package as I do not already have it\n",
    "!pip install emoji\n",
    "# Download stopwords from nltk as I have not already done that & the next cell throws an error if I do not\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine - done.\n",
    "data_location = \"/users/shailjasomani/Documents/GitHub/ADS509-tokenization-normalization/M1_Results/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here - I used the params fed into this function\n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens)) # use set to get unique list\n",
    "    lexical_diversity = num_unique_tokens/num_tokens \n",
    "    num_characters = sum(len(i) for i in tokens) # get total num of chars through all tokens\n",
    "    \n",
    "    if verbose:        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens - using param fed in again to make the num of most common tokens customizable\n",
    "        most_common_tokens = Counter(tokens).most_common(num_tokens)\n",
    "        print(f\"The {num_tokens} most common tokens are as follows: {most_common_tokens}.\")\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880fb758",
   "metadata": {},
   "source": [
    "*Note:* I was not familiar with lexical diversity prior to this assignment. Thus, I used a definition from an article in the American Journal of Speech-Language Pathology, which stated that \"one of the most commonly used approaches to measure LD is to use the ratio of unique lexical items divided by the total number of words in a sample\" (Fergadiotis, et. al., 2013).\\\n",
    "The full reference for that article is: Fergadiotis, G., Wright, H. H., & West, T. M. (2013, May). *Measuring lexical diversity in narrative discourse of people with aphasia.* American journal of speech-language pathology, 22(2), S397â€“S408. Retrieved May 18, 2024, from https://doi.org/10.1044/1058-0360(2013/12-0083)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The 13 most common tokens are as follows: [('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1), ('with', 1), ('other', 1), ('in', 1), ('this', 1)].\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: There are a variety of reasons that it's beneficial to use assertion statements in our code, including the following:\n",
    "* Check that assumptions you made remain true. For example, if you expect a parameter fed in to be a string, but it's an int, an assert condition could check if it's a string and assert an error if not (Ramos, n.d.). \n",
    "* Assertion statements also make debugging easier by triggering alarms if there is a bug and explaining where and what that bug is, based on how specific your assertion statement may be (Ramos, n.d.).\n",
    "* If you have multiple assertion statements throughout your code that verify assumptions are being met, the code is progressing as expected, etc, if there is a bug, you may be able to easier pinpoint it by knowing where the last successful checkpoint was by seeing what the last successful assertion statement was (Ramos, n.d.). \n",
    "* Assertion statements can be disabled as a group, so they can be easily disabled in a Prod environment without removing them from your code altogether. This allows them to still remain in your dev code for future testing, but not show up in your prod environment (Jones, n.d.).\n",
    "\n",
    "References\\\n",
    "Jones, D. (n.d.) *Disabling Assertions in Production.* RealPython. Retrieved May 18, 2024, from https://realpython.com/lessons/disable-python-assertions/#transcript. \\\n",
    "Ramos, L.P. (n.d.) *Python's assert: Debug and Test Your Code Like a Pro.* RealPython. Retrieved May 18, 2024, from https://realpython.com/python-assert-statement/#what-are-assertions-good-for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data the way the instructions suggest above\n",
    "# Initialize empty dictionary & path to lyrics data\n",
    "lyrics_data = {}\n",
    "lyrics_path = os.path.join(data_location, lyrics_folder)\n",
    "\n",
    "# Loop through artists in lyrics_path\n",
    "for artist in os.listdir(lyrics_path):\n",
    "    artist_path = os.path.join(lyrics_path, artist)\n",
    "    # Create nested empty dict for the \"2nd dimension\" of keys, as instructed\n",
    "    lyrics_data[artist] = {}\n",
    "    # Loop through all song file names in artist folder \n",
    "    for song_file in os.listdir(artist_path):\n",
    "        song_path = os.path.join(artist_path, song_file)\n",
    "        with open(song_path, 'r', encoding='utf-8') as file:\n",
    "            lyrics_data[artist][song_file] = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lyrics data, then clear output so not messy, long output in PDF printed\n",
    "lyrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/tgl8p9x96vl4gjdp366mpjv00000gn/T/ipykernel_59184/2679952064.py:16: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  follower_data_df = pd.read_csv(file_path, delimiter='\\t', error_bad_lines=False, warn_bad_lines=True)\n",
      "/var/folders/rq/tgl8p9x96vl4gjdp366mpjv00000gn/T/ipykernel_59184/2679952064.py:16: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  follower_data_df = pd.read_csv(file_path, delimiter='\\t', error_bad_lines=False, warn_bad_lines=True)\n",
      "b'Skipping line 624: expected 7 fields, saw 12\\nSkipping line 17506: expected 7 fields, saw 12\\nSkipping line 104621: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 188924: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 301600: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 429936: expected 7 fields, saw 12\\nSkipping line 444405: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 677792: expected 7 fields, saw 12\\nSkipping line 773482: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 818258: expected 7 fields, saw 12\\nSkipping line 895225: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 955213: expected 7 fields, saw 10\\nSkipping line 994827: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 1246039: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 1569117: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2127250: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2335031: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2681065: expected 7 fields, saw 10\\n'\n",
      "b'Skipping line 3147696: expected 7 fields, saw 12\\n'\n",
      "/var/folders/rq/tgl8p9x96vl4gjdp366mpjv00000gn/T/ipykernel_59184/2679952064.py:16: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  follower_data_df = pd.read_csv(file_path, delimiter='\\t', error_bad_lines=False, warn_bad_lines=True)\n",
      "/var/folders/rq/tgl8p9x96vl4gjdp366mpjv00000gn/T/ipykernel_59184/2679952064.py:16: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  follower_data_df = pd.read_csv(file_path, delimiter='\\t', error_bad_lines=False, warn_bad_lines=True)\n"
     ]
    }
   ],
   "source": [
    "# Read in the twitter data\n",
    "# Initialize empty dictionary & path to Twitter data\n",
    "twitter_data = {}\n",
    "twitter_path = os.path.join(data_location, twitter_folder)\n",
    "\n",
    "# Loop through file names in twitter_path \n",
    "for file_name in os.listdir(twitter_path):\n",
    "    # Keep only files that end with \"_followers_data.txt\" as they're the only one with the description col\n",
    "    if file_name.endswith(\"_followers_data.txt\"):\n",
    "        # Extract artist_name from file name\n",
    "        artist_name = file_name.replace(\"_followers_data.txt\", \"\")\n",
    "        # Get file path for relevant files (followers data files)\n",
    "        file_path = os.path.join(twitter_path, file_name)\n",
    "        \n",
    "        # Read the file into a pandas DataFrame as tab-separated to get the description col as list\n",
    "        # Was throwing errors for a few lines, so added to ignore bad lines, but warn us so we're aware which it skipped\n",
    "        follower_data_df = pd.read_csv(file_path, delimiter='\\t', error_bad_lines=False, warn_bad_lines=True)\n",
    "        descriptions = follower_data_df['description'].dropna().tolist()\n",
    "        # Store follower descriptions as list of strings with artist name as key in dict\n",
    "        twitter_data[artist_name] = descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f819a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check twitter_data, then clear output so not messy, long output in PDF printed\n",
    "twitter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36ff4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create func to clean & tokenize so can be called for both datasets\n",
    "def clean_tokenize(text):\n",
    "    # Remove punctuation chars\n",
    "    text = ''.join([char for char in text if char not in punctuation])\n",
    "    # Fold to lowercase - have to do before tokenize so string, not list\n",
    "    text = text.lower()\n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in sw]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean lyrics data here\n",
    "# Initialize new dict for cleaned lyrics data\n",
    "clean_lyrics_data = {}\n",
    "\n",
    "# Loop through all artists & songs in original lyrics data dict\n",
    "for artist, songs in lyrics_data.items():\n",
    "    # Initialize nested dict \n",
    "    clean_lyrics_data[artist] = {}\n",
    "    for song, lyrics in songs.items():\n",
    "        clean_lyrics_data[artist][song] = clean_tokenize(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b327033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean twitter data here\n",
    "# Initialize new dict for cleaned twitter data\n",
    "clean_twitter_data = {}\n",
    "\n",
    "# Loop through all artists & descriptions in original twitter data dict\n",
    "for artist, descriptions in twitter_data.items():\n",
    "    cleaned_descriptions = [clean_tokenize(description) for description in descriptions]\n",
    "    clean_twitter_data[artist] = cleaned_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "977ae1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cher', 'robynkonichiwa'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_twitter_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "29e6bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to flatten token lists, as in the Twitter data\n",
    "def flatten_token_list(token_metalist):\n",
    "    '''\n",
    "    Flattens list of list of tokens - can be nested only once.\n",
    "    Returns just a flat list of tokens.\n",
    "    '''\n",
    "    flat_token_list = []\n",
    "\n",
    "    for i in token_metalist: \n",
    "        if type(i) == list:\n",
    "            for j in i:\n",
    "                flat_token_list.append(j)\n",
    "        else:\n",
    "            flat_token_list.append(i)\n",
    "\n",
    "    return flat_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2187ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to flatten token dict of lists, as in Lyrics data\n",
    "def flatten_lyrics_dict(token_dict, artist):\n",
    "    '''\n",
    "    Flattens lyrics dict (nested dict) to flat list of tokens.\n",
    "    Relies on flatten_token_list() function.\n",
    "    '''\n",
    "    \n",
    "    flat_token_list = []\n",
    "    for song in token_dict:\n",
    "        for i in clean_lyrics_data.get(artist).get(song):\n",
    "            flat_token_list.append(i)\n",
    "\n",
    "    return flat_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0177eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16148955 tokens in the data.\n",
      "There are 1697994 unique tokens in the data.\n",
      "There are 96043813 characters in the data.\n",
      "The lexical diversity is 0.105 in the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16148955, 1697994, 0.1051457509170098, 96043813]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call descriptive_stats here for Cher Twitter data\n",
    "# Flatten Cher's list of list of tokens, then call descriptive_stats function\n",
    "cher_twitter_tokens = flatten_token_list(clean_twitter_data.get('cher'))\n",
    "descriptive_stats(cher_twitter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21912055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1538163, 271325, 0.1763954795428053, 9397180]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call descriptive_stats here for Cher Twitter data\n",
    "# Flatten Cher's list of list of tokens, then call descriptive_stats function\n",
    "robyn_twitter_tokens = flatten_token_list(clean_twitter_data.get('robynkonichiwa'))\n",
    "descriptive_stats(robyn_twitter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c08f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: \n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"â¤ï¸\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis ðŸ˜\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
